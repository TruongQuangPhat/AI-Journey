{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89435907",
   "metadata": {},
   "source": [
    "### How to build a Neural Network\n",
    "#### `Notation`\n",
    "<img src=\"image_neural_network.png\" width=400>\n",
    "\n",
    "where:\n",
    "1. Input and Activation\n",
    "   - $w^{l}_{jk}$ is weight for connection from the k - neuron in the (l - 1) - layer to the j - neuron in the l - layer.\n",
    "   - $W^l$ is weight vector for connection from the (l - 1) layer to the l layer.\n",
    "    Example: \n",
    "    $$\n",
    "    W^l = \n",
    "    \\begin{bmatrix}\n",
    "    w^{l}_{11} & w^{l}_{12} & \\cdots & w^{l}_{1k} \\\\\n",
    "    w^{l}_{21} & w^{l}_{22} & \\cdots & w^{l}_{2k} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w^{l}_{j1} & w^{l}_{j2} & \\cdots & w^{l}_{jk}\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "2. Weights\n",
    "   - $b^{l}_{k}$ is bias of the k - neuron in l - layer.\n",
    "   - $b^{l}$ is bias vector in l - layer. \n",
    "    Example:\n",
    "    $$\n",
    "    b^l = \n",
    "    \\begin{bmatrix}\n",
    "    b^{l}_{1} \\\\\n",
    "    b^{l}_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    b^{l}_{k}\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "3. Bias\n",
    "   - $a^{l}_{k}$ is activation (output) of k - neuron in l - layer.\n",
    "   - $a^{l}$ is activation vector of all neurons in l - layer.\n",
    "    Example:\n",
    "    $$\n",
    "    a^{l} = \n",
    "    \\begin{bmatrix}\n",
    "    a^{l}_{1} \\\\\n",
    "    a^{l}_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    a^{l}_{k}\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "4. Linear Combination\n",
    "   - $z^{l}_{j}$ is pre-activation of neuron j in layer l.\n",
    "   - $z^{l}$ is vector of pre-activations. \n",
    "    $$\n",
    "    z^{l} = W^{l} a^{l - 1} + b^{l}\n",
    "    $$\n",
    "5. Activation Function\n",
    "   - $a^{l}_{j} = \\sigma^{l}(z^{l}_{j})$: activation output of neuron j.\n",
    "   - $a^{l} = \\sigma^{l}(z^{l})$: vectorized activation.\n",
    "6. Output\n",
    "   - $a^{L} = \\hat{y}$: prediction at output layer L.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740e60b",
   "metadata": {},
   "source": [
    "#### `Step 1: Parameter Initialization`\n",
    "While building and training neural networks, it is crucial to initialize the weights appropriately to ensure a model with high accuracy. If the weights are not correctly initialized, it may give rise to the Vanishing Gradient problem or the Exploding Gradient problem.\n",
    "\n",
    "`1. Zero Initialization`\n",
    "- As the name suggests, all the weights are assigned zero as the initial value is zero initialization. This kind of initialization is highly ineffective as neurons learn the same feature during each iteration. Rather, during any kind of constant initialization, the same issue happens to occur. Thus, constant initializations are not preferred.\n",
    "\n",
    "`2. Random Initialization`\n",
    "- In an attempt to overcome the shortcomings of Zero or Constant Initialization, random initialization assigns random values except for zeros as weights to neuron paths. However, assigning values randomly to the weights, problems such as **Overfitting, Vanishing Gradient Problem, Exploding Gradient Problem** might occur. \n",
    "- Random Initialization can be of two kinds:\n",
    "   - `2.1 Random Normal`: The weights are initialized from values in a normal distribution.\n",
    "    $$\n",
    "    w_i \\sim N(0, 1)\n",
    "    $$\n",
    "    - `2.2 Random Uniform`: The weights are initialized from values in a uniform distribution.\n",
    "    $$\n",
    "    w_i \\sim U[min, max]\n",
    "    $$\n",
    "`3. Xavier/Glorot Initialization`\n",
    "  - `3.1 Xavier Uniform`: Xavier/Glorot initialization often termed as Xavier Uniform initialization, is suitable for layers where the activation function used is **Sigmoid**.\n",
    "    $$\n",
    "    w_i \\sim U[-\\sqrt{\\frac{6}{fan\\_in + fan\\_out}}, \\sqrt{\\frac{6}{fan\\_in + fan\\_out}}]\n",
    "    $$\n",
    "  - `3.2 Xevier Normal`: Xavier/Glorot Initialization, too, is suitable for layers where the activation function used is **Sigmoid**.\n",
    "    $$\n",
    "    w_i \\sim N(0, \\sigma)\n",
    "    $$\n",
    "    Here, $\\sigma$ is given by:\n",
    "    $$\n",
    "    \\sigma = \\sqrt{\\frac{6}{fan\\_in + fan\\_out}}\n",
    "    $$\n",
    "`4. He Initialization`\n",
    "  - `4.1 He Uniform`: He Uniform Initialization is suitable for layers where ReLU activation function is used.\n",
    "    $$\n",
    "    w_i \\sim U[-\\sqrt{\\frac{6}{fan\\_in}}, \\sqrt{\\frac{6}{fan\\_out}}]\n",
    "    $$\n",
    "  - `4.2 He Normal`: He Uniform Initialization, too, is suitable for layers where ReLU activation function is used.\n",
    "    $$\n",
    "    w_i \\sim N(0, \\sigma)\n",
    "    $$\n",
    "    Here, $\\sigma$ is given by:\n",
    "    $$\n",
    "    \\sigma = \\sqrt{\\frac{2}{fan\\_in}}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6b657d",
   "metadata": {},
   "source": [
    "#### `Step 2: Forward Propagation`\n",
    "- Input: X\n",
    "- For each layer l = 1,...,L:\n",
    "$$\n",
    "z^{l} = W^{l}a^{l - 1} + b^{l}, a^{l} = \\sigma^{l}(z^{l})\n",
    "$$\n",
    "- $a^{0} = X$\n",
    "- Final output: \n",
    "$$\n",
    "a^{L} = \\hat{y}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa2497",
   "metadata": {},
   "source": [
    "#### `Step 3: Compute Cost Function`\n",
    "- Cost Function:\n",
    "$$\n",
    "C = \\frac{1}{2N} \\sum_{x}^{N}\\left( y(x) - a^{L}(x) \\right)^2\n",
    "$$\n",
    "- For one input sample:\n",
    "$$\n",
    "C = \\frac{1}{2}(y - a^{L})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225de3a6",
   "metadata": {},
   "source": [
    "#### `Step 4: Backpropagation`\n",
    "- We need to find: \n",
    "$$\n",
    "\\Large\n",
    "\\begin{cases}\n",
    "\\frac{\\partial C}{\\partial w^{l}_{jk}} \\\\\n",
    "\\frac{\\partial C}{\\partial b^{l}_{j}}\n",
    "\\end{cases}\n",
    "$$\n",
    "`NOTE`: In backpropagation, all values $a^{l}_{j}, z^{l}_{j}$ and y are available.\n",
    "- We have: \n",
    "    $$\n",
    "    \\frac{\\partial C}{\\partial w^{l}_{jk}} = \\frac{\\partial C}{\\partial z^{l}_{j}}\\frac{\\partial z^{l}_{j}}{\\partial w^{l}_{jk}}\n",
    "    $$\n",
    "    And\n",
    "    $$\n",
    "    \\large\n",
    "    z^{l}_{j} = \\sum_{k}w^{l}_{jk}.a^{l - 1}_{k} + b^{l}_{j} = w^{l}_{j1}.a^{l - 1}_{1} + w^{l}_{j2}.a^{l - 1}_{2} + ... + w^{l}_{jk}.a^{l - 1}_{k} + b^{l}_{j}\n",
    "    $$\n",
    "    Thus\n",
    "    $$\n",
    "    \\large\n",
    "    \\frac{\\partial z^{l}_{j}}{\\partial w^{l}_{jk}} = a^{l - 1}_{k}\n",
    "    $$\n",
    "    Therefore\n",
    "    $$\n",
    "    \\Large\n",
    "    \\begin{cases}\n",
    "    \\frac{\\partial C}{\\partial w^{l}_{jk}} = \\frac{\\partial C}{\\partial z^{l}_{j}}a^{l - 1}_{k} \\\\\n",
    "    \\frac{\\partial C}{\\partial w^{l}_{j}} = \\frac{\\partial C}{\\partial z^{l}_{j}}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "- We define the error $\\delta^{l}_{j}$ neuron j in layer l by:\n",
    "    $$\n",
    "    \\large\n",
    "    \\delta^{l}_{j} := \\frac{\\partial C}{\\partial z^{l}_{j}}\n",
    "    $$\n",
    "    Therefore\n",
    "    $$\n",
    "    \\Large\n",
    "    \\begin{cases}\n",
    "    \\frac{\\partial C}{\\partial w^{l}_{jk}} = \\delta^{l}_{j} a^{l - 1}_{k} \\\\\n",
    "    \\frac{\\partial C}{\\partial w^{l}_{j}} = \\delta^{l}_{j}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    we have\n",
    "    $$\n",
    "    \\large\n",
    "    a^{l}_{j} = \\sigma(z^{l}_{j})\n",
    "    $$\n",
    "    Hence\n",
    "    $$\n",
    "    \\large\n",
    "    \\delta^{L}_{j} = \\frac{\\partial C}{\\partial z^{L}_{j}} = \\frac{\\partial C}{\\partial a^{L}_{j}} \\frac{\\partial a^{L}_{j}}{\\partial z^{L}_{j}} = \\frac{\\partial C}{\\partial a^{L}_{j}} \\sigma'(z^{L}_{j}) \\\\\n",
    "    $$\n",
    "- `Problem for the previous layers`: $\\Large \\frac{\\partial C}{\\partial a^{l - 1}_{j}}, \\frac{\\partial C}{\\partial a^{l - 2}_{j}},...$\n",
    "    - We don't have direct ralation between C and $a^{l - 1}_{j}$, so we need to find $\\large \\delta^{l-1} = f(\\delta^{l})$\n",
    "- `Compute` $\\large \\delta^{l - 1}_{j}$\n",
    "    - We have\n",
    "    $$\n",
    "    \\large\n",
    "    \\delta^{l - 1}_{j} = \\frac{\\partial C}{\\partial z^{l - 1}_{j}}\n",
    "    $$\n",
    "    - Neuron j of layer l - 1 influences all neurons in layer l\n",
    "    $$\n",
    "    \\delta^{l - 1}_{j} = \\frac{\\partial C}{\\partial z^{l - 1}_{j}} = \\frac{\\partial C}{\\partial z^{l}_{1}} \\frac{\\partial z^{l}_{1}}{\\partial z^{l - 1}_{j}} + \\frac{\\partial C}{\\partial z^{l}_{2}} \\frac{\\partial z^{l}_{2}}{\\partial z^{l - 1}_{j}} + ... + \\frac{\\partial C}{\\partial z^{l}_{k}} \\frac{\\partial z^{l}_{k}}{\\partial z^{l - 1}_{j}} = \\sum_{k} \\frac{\\partial C}{\\partial z^{l}_{k}} \\frac{\\partial z^{l}_{k}}{\\partial z^{l - 1}_{j}}\n",
    "    $$\n",
    "    $$\n",
    "    \\large\n",
    "    \\Rightarrow\n",
    "    \\delta^{l - 1}_{j} = \\sum_{k} \\delta^{l}_{k} \\frac{\\partial z^{l}_{k}}{\\partial z^{l - 1}_{j}}\n",
    "    $$\n",
    "    And\n",
    "    $$\n",
    "    \\large\n",
    "    z^{l}_{k} = \\sum_{j} w^{l}_{kj}.a^{l - 1}_{j} + b^{l}_{k} = \\sum_{j} w^{l}_{kj}.\\sigma(z^{l - 1}_{j}) + b^{l}_{k}\n",
    "    \\Rightarrow \\frac{\\partial z^{l}_{k}}{\\partial z^{l - 1}_{j}} = w^{l}_{kj}.\\sigma'(z^{l - 1}_{j})\n",
    "    $$\n",
    "    Thus\n",
    "    $$\n",
    "    \\large\n",
    "    \\delta^{l - 1}_{j} = \\sum_{k} \\delta^{l}_{k} w^{l}_{kj}.\\sigma'(z^{l - 1}_{j})\n",
    "    $$\n",
    "    $$\n",
    "    \\large\n",
    "    \\Leftrightarrow \\delta^{l - 1}_{j} = [\\sum_{k} w^{l}_{kj}.\\delta^{l}_{k}] \\sigma'(z^{l - 1}_{j})\n",
    "    $$\n",
    "    For the (l - 1) layer\n",
    "    $$\n",
    "    \\large\n",
    "    \\begin{bmatrix}\n",
    "    \\delta^{l - 1}_{1} \\\\\n",
    "    \\delta^{l - 1}_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\delta^{l - 1}_{j}\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "    \\sum_{k} w^{l}_{k1} . \\delta^{l}_{k} \\\\\n",
    "    \\sum_{k} w^{l}_{k2} . \\delta^{l}_{k} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sum_{k} w^{l}_{kj} . \\delta^{l}_{k} \\\\\n",
    "    \\end{bmatrix}\n",
    "    \\;\\odot\\;\n",
    "    \\begin{bmatrix}\n",
    "    \\sigma'(z^{l - 1}_{1}) \\\\\n",
    "    \\sigma'(z^{l - 1}_{2}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sigma'(z^{l - 1}_{j}) \\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "    $$\n",
    "    \\large\n",
    "    \\Leftrightarrow\n",
    "    \\begin{bmatrix}\n",
    "    \\delta^{l - 1}_{1} \\\\\n",
    "    \\delta^{l - 1}_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\delta^{l - 1}_{j}\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "    w^{l}_{11} . \\delta^{l}_{1} + w^{l}_{21} . \\delta^{l}_{2} + ... + w^{l}_{k1} . \\delta^{l}_{k} \\\\\n",
    "    w^{l}_{12} . \\delta^{l}_{1} + w^{l}_{22} . \\delta^{l}_{2} + ... + w^{l}_{k2} . \\delta^{l}_{k} \\\\\n",
    "    \\vdots \\\\\n",
    "    w^{l}_{1j} . \\delta^{l}_{1} + w^{l}_{2j} . \\delta^{l}_{2} + ... + w^{l}_{kj} . \\delta^{l}_{k}\n",
    "    \\end{bmatrix}\n",
    "    \\;\\odot\\;\n",
    "    \\begin{bmatrix}\n",
    "    \\sigma'(z^{l - 1}_{1}) \\\\\n",
    "    \\sigma'(z^{l - 1}_{2}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sigma'(z^{l - 1}_{j}) \\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "    $$\n",
    "    \\large\n",
    "    \\Leftrightarrow\n",
    "    \\begin{bmatrix}\n",
    "    \\delta^{l - 1}_{1} \\\\\n",
    "    \\delta^{l - 1}_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\delta^{l - 1}_{j}\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "    w^{l}_{11} & w^{l}_{21} & ... & w^{l}_{k1} \\\\\n",
    "    w^{l}_{12} & w^{l}_{22} & ... & w^{l}_{k2}\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w^{l}_{1j} & w^{l}_{2j} & ... & w^{l}_{kj}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    \\delta^{l}_{1} \\\\\n",
    "    \\delta^{l}_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\delta^{l}_{k}\n",
    "    \\end{bmatrix}\n",
    "    \\;\\odot\\;\n",
    "    \\begin{bmatrix}\n",
    "    \\sigma'(z^{l - 1}_{1}) \\\\\n",
    "    \\sigma'(z^{l - 1}_{2}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sigma'(z^{l - 1}_{j}) \\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "    $$\n",
    "    \\large\n",
    "    \\Leftrightarrow\n",
    "    \\delta^{l - 1} = (W^{l})^{T} \\delta^{l} \\odot \\sigma'(z^{l - 1})\n",
    "    $$\n",
    "#### `Step 5: Gradient Descent`\n",
    "- `Gradient`:\n",
    "    $$\n",
    "    \\large\n",
    "    \\begin{cases}\n",
    "    \\frac{\\partial C}{\\partial w^{l}_{jk}} = \\delta^{l}_{j} a^{l - 1}_{k} \\\\\n",
    "    \\frac{\\partial C}{\\partial b^{l}_{j}} = \\delta^{l}_{j}\n",
    "    \\end{cases}\n",
    "    \\Rightarrow\n",
    "    \\begin{cases}\n",
    "    \\frac{\\partial C}{\\partial W^{l}} = \\delta^{l} (a^{l - 1})^T \\\\\n",
    "    \\frac{\\partial C}{\\partial b^{l}} = \\delta^{l}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "- `Update`:\n",
    "    $$\n",
    "    \\large\n",
    "    \\begin{cases}\n",
    "    W^{l} := W^{l} - \\alpha \\frac{\\partial C}{\\partial W^{l}} = W^{l} - \\alpha \\delta^{l}(a^{l - 1})^T \\\\\n",
    "    b^{l} := b^{l} - \\alpha \\frac{\\partial C}{\\partial b^{l}} = b^{l} - \\alpha \\delta^{l}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "- `With m samples`:\n",
    "    $$\n",
    "    \\large\n",
    "    \\begin{cases}\n",
    "    \\frac{\\partial C}{\\partial W^{l}} = \\frac{1}{m} \\delta^{l} (a^{l - 1})^{T} \\\\\n",
    "    \\frac{\\partial C}{\\partial b^{l}} = \\frac{1}{m} \\delta^{l}\n",
    "    \\end{cases}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105315c",
   "metadata": {},
   "source": [
    "### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "23921ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "7e00312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivate(x):\n",
    "        s = Activation.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh_derivate(x):\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivate(x):\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "ad7c8af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, y_true, y_pred):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, y_true, y_pred):\n",
    "        pass\n",
    "\n",
    "class MeanSquaredError(LossFunction):\n",
    "    def forward(self, y_true, y_pred):\n",
    "        return np.mean(np.square(y_pred - y_true))\n",
    "    \n",
    "    def backward(self, y_true, y_pred):\n",
    "        return 2 * (y_pred - y_true) / y_true.shape[1]\n",
    "    \n",
    "class CrossEntropy(LossFunction):\n",
    "    def forward(self, y_true, y_pred):\n",
    "        m = y_true.shape[1]\n",
    "        eps = 1e-12\n",
    "        y_pred_clipped = np.clip(y_pred, eps, 1 - eps)\n",
    "        return -np.sum(y_true * np.log(y_pred_clipped)) / m\n",
    "\n",
    "    def backward(self, y_true, y_pred):\n",
    "        return (y_pred - y_true) / y_true.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "9ffa96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation=\"sigmoid\"):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.W, self.b = self.init_weights(input_size, output_size, activation)\n",
    "\n",
    "        self.z = None   # z^l = W x + b, shape (fan_out, m)\n",
    "        self.a = None   # a^l = f(z^l), shape (fan_out, m)\n",
    "        self.x = None   # x = a^{l-1}, shape (fan_in, m)\n",
    "\n",
    "        # activation\n",
    "        self.activation = activation\n",
    "\n",
    "    def activate(self, z):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return Activation.sigmoid(z)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return Activation.tanh(z)\n",
    "        elif self.activation == \"relu\":\n",
    "            return Activation.relu(z)\n",
    "        elif self.activation == \"softmax\":\n",
    "            return Activation.softmax(z)\n",
    "        else:\n",
    "            return z  # linear   \n",
    "\n",
    "    def activate_derivative(self, z):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return Activation.sigmoid_derivate(z)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return Activation.tanh_derivate(z)\n",
    "        elif self.activation == \"relu\":\n",
    "            return Activation.relu_derivate(z)\n",
    "        else:\n",
    "            return 1 # linear    \n",
    "    \n",
    "    def init_weights(self, fan_in, fan_out, activation):\n",
    "        if activation in [\"sigmoid\", \"tanh\", \"softmax\", \"linear\"]:\n",
    "            # Xavier initialization\n",
    "            limit = np.sqrt(6 / (fan_in + fan_out))\n",
    "            W = np.random.uniform(-limit, limit, size=(fan_out, fan_in))\n",
    "        elif activation in [\"relu\", \"leaky_relu\"]:\n",
    "            # He initialization\n",
    "            std = np.sqrt(2 / fan_in)\n",
    "            W = np.random.randn(fan_out, fan_in) * std\n",
    "        else:\n",
    "            # default small random\n",
    "            W = np.random.randn(fan_out, fan_in) * 0.01\n",
    "        b = np.zeros((fan_out, 1))\n",
    "        return W, b\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.z = np.dot(self.W, x) + self.b\n",
    "        self.a = self.activate(self.z)\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, delta, learning_rate):\n",
    "        m = self.x.shape[1] # samples\n",
    "\n",
    "        delta = delta * self.activate_derivative(self.z) # delta * da/dz (if layer is L, da/dz = 1 (softmax))\n",
    "\n",
    "        dW = np.dot(delta, self.x.T) / m\n",
    "        db = np.sum(delta, axis=1, keepdims=True) / m\n",
    "        prev_delta = np.dot(self.W.T, delta)\n",
    "\n",
    "        # Update weights\n",
    "        self.W -= learning_rate * dW\n",
    "        self.b -= learning_rate * db\n",
    "\n",
    "        return prev_delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "c3ea865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, loss=\"mse\", learning_rate=0.1):\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        if loss == \"mse\":\n",
    "            self.loss = MeanSquaredError()\n",
    "        elif loss == \"crossentropy\":\n",
    "            self.loss = CrossEntropy()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown loss function\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a = x\n",
    "        for layer in self.layers:\n",
    "            a = layer.forward(a)\n",
    "        return a\n",
    "\n",
    "    def backward(self, y_true, y_pred):\n",
    "        m = y_true.shape[1] # samples\n",
    "        last_layer = self.layers[-1] \n",
    "\n",
    "        # CrossEntropy + softmax/sigmoid\n",
    "        if isinstance(self.loss, CrossEntropy) and last_layer.activation in (\"softmax\", \"sigmoid\"):\n",
    "            delta = (y_pred - y_true) / m # delta^L\n",
    "        else:\n",
    "            dC_da = self.loss.backward(y_true, y_pred) # dC/da^L\n",
    "            delta = dC_da \n",
    "\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            delta = self.layers[i].backward(delta, self.learning_rate)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def evaluate(self, X, Y):\n",
    "        y_pred = self.forward(X)\n",
    "        if y_pred.shape[0] == 1:\n",
    "            preds = (y_pred > 0.5).astype(int)\n",
    "            acc = np.mean(preds == Y)\n",
    "        else:\n",
    "            preds = np.argmax(y_pred, axis=0)\n",
    "            labels = np.argmax(Y, axis=0)\n",
    "            acc = np.mean(preds == labels)\n",
    "        return acc\n",
    "    \n",
    "    def train(self, X, Y, epochs=1000, verbose=100):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X) # y_pred = a^L\n",
    "            loss_value = self.loss.forward(Y, y_pred) # Cost\n",
    "            self.backward(Y, y_pred)\n",
    "\n",
    "            if verbose and epoch % verbose == 0:\n",
    "                acc = self.evaluate(X, Y)\n",
    "                print(f\"Epoch {epoch}:  Loss = {loss_value:.6f}, Accuracy = {acc:.4}\")\n",
    "        print(f\"Epoch {epochs}:  Loss = {loss_value:.6f}, Accuracy = {acc:.4}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5cb92a",
   "metadata": {},
   "source": [
    "### Iris Flower Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "fd2e356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "36e32b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "2a4f12fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "iris = load_iris()\n",
    "X = iris.data # (150, 4)\n",
    "y = iris.target.reshape(-1, 1) # (150, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "8fac59ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "1c5455e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_test = X_train.T, X_test.T \n",
    "y_train, y_test = y_train.T, y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "174fa7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (4, 120) (3, 120)\n",
      "Test shape:  (4, 30) (3, 30)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape: \", X_train.shape, y_train.shape)\n",
    "print(\"Test shape: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "c8f972ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Layer(4, 25, \"relu\"),\n",
    "    Layer(25, 16, \"relu\"),\n",
    "    Layer(16, 8, \"relu\"),\n",
    "    Layer(8, 3, \"softmax\")\n",
    "]\n",
    "\n",
    "nn = NeuralNetwork(layers, loss=\"crossentropy\", learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "17425121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  Loss = 1.652891, Accuracy = 0.1083\n",
      "Epoch 1000:  Loss = 0.695612, Accuracy = 0.675\n",
      "Epoch 2000:  Loss = 0.589571, Accuracy = 0.7583\n",
      "Epoch 3000:  Loss = 0.507844, Accuracy = 0.8\n",
      "Epoch 4000:  Loss = 0.423265, Accuracy = 0.825\n",
      "Epoch 5000:  Loss = 0.352607, Accuracy = 0.85\n",
      "Epoch 6000:  Loss = 0.291419, Accuracy = 0.9\n",
      "Epoch 7000:  Loss = 0.245401, Accuracy = 0.925\n",
      "Epoch 8000:  Loss = 0.209302, Accuracy = 0.9417\n",
      "Epoch 9000:  Loss = 0.179833, Accuracy = 0.9667\n",
      "Epoch 10000:  Loss = 0.155966, Accuracy = 0.9667\n"
     ]
    }
   ],
   "source": [
    "nn.train(X_train, y_train, epochs=10000, verbose=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "0b56b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = nn.evaluate(X_train, y_train)\n",
    "acc_test = nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "94032c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9667\n",
      "Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Accuracy: {acc_train:.4f}\")\n",
    "print(f\"Test Accuracy: {acc_test:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
