{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89435907",
   "metadata": {},
   "source": [
    "### How to build a Neural Network\n",
    "#### `Notation`\n",
    "<img src=\"image_neural_network.png\" width=400>\n",
    "\n",
    "where:\n",
    "1. Input and Activation\n",
    "   - $w^{l}_{jk}$ is weight for connection from the k - neuron in the (l - 1) - layer to the j - neuron in the l - layer.\n",
    "   - $W^l$ is weight vector for connection from the (l - 1) layer to the l layer.\n",
    "    Example: \n",
    "    $$\n",
    "    W^l = \n",
    "    \\begin{bmatrix}\n",
    "    w^{l}_{11} & w^{l}_{12} & \\cdots & w^{l}_{1k} \\\\\n",
    "    w^{l}_{21} & w^{l}_{22} & \\cdots & w^{l}_{2k} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w^{l}_{j1} & w^{l}_{j2} & \\cdots & w^{l}_{jk}\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "2. Weights\n",
    "   - $b^{l}_{k}$ is bias of the k - neuron in l - layer.\n",
    "   - $b^{l}$ is bias vector in l - layer. \n",
    "    Example:\n",
    "    $$\n",
    "    b^l = \n",
    "    \\begin{bmatrix}\n",
    "    b^{l}_{1} \\\\\n",
    "    b^{l}_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    b^{l}_{k}\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "3. Bias\n",
    "   - $a^{l}_{k}$ is activation (output) of k - neuron in l - layer.\n",
    "   - $a^{l}$ is activation vector of all neurons in l - layer.\n",
    "    Example:\n",
    "    $$\n",
    "    a^{l} = \n",
    "    \\begin{bmatrix}\n",
    "    a^{l}_{1} \\\\\n",
    "    a^{l}_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    a^{l}_{k}\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "4. Linear Combination\n",
    "   - $z^{l}_{j}$ is pre-activation of neuron j in layer l.\n",
    "   - $z^{l}$ is vector of pre-activations. \n",
    "    $$\n",
    "    z^{l} = W^{l} a^{l - 1} + b^{l}\n",
    "    $$\n",
    "5. Activation Function\n",
    "   - $a^{l}_{j} = \\sigma^{l}(z^{l}_{j})$: activation output of neuron j.\n",
    "   - $a^{l} = \\sigma^{l}(z^{l})$: vectorized activation.\n",
    "6. Output\n",
    "   - $a^{L} = \\hat{y}$: prediction at output layer L.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740e60b",
   "metadata": {},
   "source": [
    "#### `Step 1: Parameter Initialization`\n",
    "While building and training neural networks, it is crucial to initialize the weights appropriately to ensure a model with high accuracy. If the weights are not correctly initialized, it may give rise to the Vanishing Gradient problem or the Exploding Gradient problem.\n",
    "\n",
    "`1. Zero Initialization`\n",
    "- As the name suggests, all the weights are assigned zero as the initial value is zero initialization. This kind of initialization is highly ineffective as neurons learn the same feature during each iteration. Rather, during any kind of constant initialization, the same issue happens to occur. Thus, constant initializations are not preferred.\n",
    "\n",
    "`2. Random Initialization`\n",
    "- In an attempt to overcome the shortcomings of Zero or Constant Initialization, random initialization assigns random values except for zeros as weights to neuron paths. However, assigning values randomly to the weights, problems such as **Overfitting, Vanishing Gradient Problem, Exploding Gradient Problem** might occur. \n",
    "- Random Initialization can be of two kinds:\n",
    "   - `2.1 Random Normal`: The weights are initialized from values in a normal distribution.\n",
    "    $$\n",
    "    w_i \\sim N(0, 1)\n",
    "    $$\n",
    "    - `2.2 Random Uniform`: The weights are initialized from values in a uniform distribution.\n",
    "    $$\n",
    "    w_i \\sim U[min, max]\n",
    "    $$\n",
    "`3. Xavier/Glorot Initialization`\n",
    "  - `3.1 Xavier Uniform`: Xavier/Glorot initialization often termed as Xavier Uniform initialization, is suitable for layers where the activation function used is **Sigmoid**.\n",
    "    $$\n",
    "    w_i \\sim U[-\\sqrt{\\frac{6}{fan\\_in + fan\\_out}}, \\sqrt{\\frac{6}{fan\\_in + fan\\_out}}]\n",
    "    $$\n",
    "  - `3.2 Xevier Normal`: Xavier/Glorot Initialization, too, is suitable for layers where the activation function used is **Sigmoid**.\n",
    "    $$\n",
    "    w_i \\sim N(0, \\sigma)\n",
    "    $$\n",
    "    Here, $\\sigma$ is given by:\n",
    "    $$\n",
    "    \\sigma = \\sqrt{\\frac{6}{fan\\_in + fan\\_out}}\n",
    "    $$\n",
    "`4. He Initialization`\n",
    "  - `4.1 He Uniform`: He Uniform Initialization is suitable for layers where ReLU activation function is used.\n",
    "    $$\n",
    "    w_i \\sim U[-\\sqrt{\\frac{6}{fan\\_in}}, \\sqrt{\\frac{6}{fan\\_out}}]\n",
    "    $$\n",
    "  - `4.2 He Normal`: He Uniform Initialization, too, is suitable for layers where ReLU activation function is used.\n",
    "    $$\n",
    "    w_i \\sim N(0, \\sigma)\n",
    "    $$\n",
    "    Here, $\\sigma$ is given by:\n",
    "    $$\n",
    "    \\sigma = \\sqrt{\\frac{2}{fan\\_in}}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6b657d",
   "metadata": {},
   "source": [
    "#### `Step 2: Forward Propagation`\n",
    "- Input: X\n",
    "- For each layer l = 1,...,L:\n",
    "$$\n",
    "z^{l} = W^{l}a^{l - 1} + b^{l}, a^{l} = \\sigma^{l}(z^{l})\n",
    "$$\n",
    "- $a^{0} = X$\n",
    "- Final output: \n",
    "$$\n",
    "a^{L} = \\hat{y}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa2497",
   "metadata": {},
   "source": [
    "#### `Step 3: Compute Cost Function`\n",
    "- Cost Function:\n",
    "$$\n",
    "C = \\frac{1}{2N} \\sum_{x}^{N}\\left( y(x) - a^{L}(x) \\right)^2\n",
    "$$\n",
    "- For one input sample:\n",
    "$$\n",
    "C = \\frac{1}{2}(y - a^{L})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225de3a6",
   "metadata": {},
   "source": [
    "#### `Step 4: Backpropagation`\n",
    "- We need to find: \n",
    "$$\n",
    "\\Large\n",
    "\\begin{cases}\n",
    "\\frac{\\partial C}{\\partial w^{l}_{jk}} \\\\\n",
    "\\frac{\\partial C}{\\partial b^{l}_{j}}\n",
    "\\end{cases}\n",
    "$$\n",
    "`NOTE`: In backpropagation, all values $a^{l}_{j}, z^{l}_{j}$ and y are available.\n",
    "- We have: \n",
    "    $$\n",
    "    \\frac{\\partial C}{\\partial w^{l}_{jk}} = \\frac{\\partial C}{\\partial z^{l}_{j}}\\frac{\\partial z^{l}_{j}}{\\partial w^{l}_{jk}}\n",
    "    $$\n",
    "    And\n",
    "    $$\n",
    "    \\large\n",
    "    z^{l}_{j} = \\sum_{k}w^{l}_{jk}.a^{l - 1}_{k} + b^{l}_{j} = w^{l}_{j1}.a^{l - 1}_{1} + w^{l}_{j2}.a^{l - 1}_{2} + ... + w^{l}_{jk}.a^{l - 1}_{k} + b^{l}_{j}\n",
    "    $$\n",
    "    Thus\n",
    "    $$\n",
    "    \\large\n",
    "    \\frac{\\partial z^{l}_{j}}{\\partial w^{l}_{jk}} = a^{l - 1}_{k}\n",
    "    $$\n",
    "    Therefore\n",
    "    $$\n",
    "    \\Large\n",
    "    \\begin{cases}\n",
    "    \\frac{\\partial C}{\\partial w^{l}_{jk}} = \\frac{\\partial C}{\\partial z^{l}_{j}}a^{l - 1}_{k} \\\\\n",
    "    \\frac{\\partial C}{\\partial w^{l}_{j}} = \\frac{\\partial C}{\\partial z^{l}_{j}}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "- We define the error $\\delta^{l}_{j}$ neuron j in layer l by:\n",
    "    $$\n",
    "    \\large\n",
    "    \\delta^{l}_{j} := \\frac{\\partial C}{\\partial z^{l}_{j}}\n",
    "    $$\n",
    "    Therefore\n",
    "    $$\n",
    "    \\Large\n",
    "    \\begin{cases}\n",
    "    \\frac{\\partial C}{\\partial w^{l}_{jk}} = \\delta^{l}_{j} a^{l - 1}_{k} \\\\\n",
    "    \\frac{\\partial C}{\\partial w^{l}_{j}} = \\delta^{l}_{j}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    we have\n",
    "    $$\n",
    "    \\large\n",
    "    a^{l}_{j} = \\sigma(z^{l}_{j})\n",
    "    $$\n",
    "    Hence\n",
    "    $$\n",
    "    \\large\n",
    "    \\delta^{l}_{j} = \\frac{\\partial C}{\\partial z^{l}_{j}} = \\frac{\\partial C}{\\partial a^{l}_{j}} \\frac{\\partial a^{l}_{j}}{\\partial z^{l}_{j}} = \\frac{\\partial C}{\\partial a^{l}_{j}} \\sigma'(z^{l}_{j}) \\\\\n",
    "    $$\n",
    "- `Problem for the previous layers`: $\\Large \\frac{\\partial C}{\\partial a^{l - 1}_{j}}, \\frac{\\partial C}{\\partial a^{l - 2}_{j}},...$\n",
    "    - We don't have direct ralation between C and $a^{l - 1}_{j}$, so we need to find $\\large \\delta^{l-1} = f(\\delta^{l})$\n",
    "- `Compute` $\\large \\delta^{l - 1}_{j}$\n",
    "    - We have\n",
    "    $$\n",
    "    \\large\n",
    "    \\delta^{l - 1}_{j} = \\frac{\\partial C}{\\partial z^{l - 1}_{j}}\n",
    "    $$\n",
    "    - Neuron j of layer l - 1 influences all neurons in layer l\n",
    "    $$\n",
    "    \\delta^{l - 1}_{j} = \\frac{\\partial C}{\\partial z^{l - 1}_{j}} = \\frac{\\partial C}{\\partial z^{l}_{1}} \\frac{\\partial z^{l}_{1}}{\\partial z^{l - 1}_{j}} + \\frac{\\partial C}{\\partial z^{l}_{2}} \\frac{\\partial z^{l}_{2}}{\\partial z^{l - 1}_{j}} + ... + \\frac{\\partial C}{\\partial z^{l}_{k}} \\frac{\\partial z^{l}_{k}}{\\partial z^{l - 1}_{j}} = \\sum_{k} \\frac{\\partial C}{\\partial z^{l}_{k}} \\frac{\\partial z^{l}_{k}}{\\partial z^{l - 1}_{j}}\n",
    "    $$\n",
    "    $$\n",
    "    \\large\n",
    "    \\Rightarrow\n",
    "    \\delta^{l - 1}_{j} = \\sum_{k} \\delta^{l}_{k} \\frac{\\partial z^{l}_{k}}{\\partial z^{l - 1}_{j}}\n",
    "    $$\n",
    "    And\n",
    "    $$\n",
    "    \\large\n",
    "    z^{l}_{k} = \\sum_{j} w^{l}_{kj}.a^{l - 1}_{j} + b^{l}_{k} = \\sum_{j} w^{l}_{kj}.\\sigma(z^{l - 1}_{j}) + b^{l}_{k}\n",
    "    \\Rightarrow \\frac{\\partial z^{l}_{k}}{\\partial z^{l - 1}_{j}} = w^{l}_{kj}.\\sigma'(z^{l - 1}_{j})\n",
    "    $$\n",
    "    Thus\n",
    "    $$\n",
    "    \\large\n",
    "    \\delta^{l - 1}_{j} = \\sum_{k} \\delta^{l}_{k} w^{l}_{kj}.\\sigma'(z^{l - 1}_{j})\n",
    "    $$\n",
    "    $$\n",
    "    \\large\n",
    "    \\Leftrightarrow \\delta^{l - 1}_{j} = [\\sum_{k} w^{l}_{kj}.\\delta^{l}_{k}] \\sigma'(z^{l - 1}_{j})\n",
    "    $$\n",
    "    For the (l - 1) layer\n",
    "    $$\n",
    "    \\large\n",
    "    \\begin{bmatrix}\n",
    "    \\delta^{l - 1}_{1} \\\\\n",
    "    \\delta^{l - 1}_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\delta^{l - 1}_{j}\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "    \\sum_{k} w^{l}_{k1} . \\delta^{l}_{k} \\\\\n",
    "    \\sum_{k} w^{l}_{k2} . \\delta^{l}_{k} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sum_{k} w^{l}_{kj} . \\delta^{l}_{k} \\\\\n",
    "    \\end{bmatrix}\n",
    "    \\;\\odot\\;\n",
    "    \\begin{bmatrix}\n",
    "    \\sigma'(z^{l - 1}_{1}) \\\\\n",
    "    \\sigma'(z^{l - 1}_{2}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sigma'(z^{l - 1}_{j}) \\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "    $$\n",
    "    \\large\n",
    "    \\Leftrightarrow\n",
    "    \\begin{bmatrix}\n",
    "    \\delta^{l - 1}_{1} \\\\\n",
    "    \\delta^{l - 1}_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\delta^{l - 1}_{j}\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "    w^{l}_{11} . \\delta^{l}_{1} + w^{l}_{21} . \\delta^{l}_{2} + ... + w^{l}_{k1} . \\delta^{l}_{k} \\\\\n",
    "    w^{l}_{12} . \\delta^{l}_{1} + w^{l}_{22} . \\delta^{l}_{2} + ... + w^{l}_{k2} . \\delta^{l}_{k} \\\\\n",
    "    \\vdots \\\\\n",
    "    w^{l}_{1j} . \\delta^{l}_{1} + w^{l}_{2j} . \\delta^{l}_{2} + ... + w^{l}_{kj} . \\delta^{l}_{k}\n",
    "    \\end{bmatrix}\n",
    "    \\;\\odot\\;\n",
    "    \\begin{bmatrix}\n",
    "    \\sigma'(z^{l - 1}_{1}) \\\\\n",
    "    \\sigma'(z^{l - 1}_{2}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sigma'(z^{l - 1}_{j}) \\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "    $$\n",
    "    \\large\n",
    "    \\Leftrightarrow\n",
    "    \\begin{bmatrix}\n",
    "    \\delta^{l - 1}_{1} \\\\\n",
    "    \\delta^{l - 1}_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\delta^{l - 1}_{j}\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "    w^{l}_{11} & w^{l}_{21} & ... & w^{l}_{k1} \\\\\n",
    "    w^{l}_{12} & w^{l}_{22} & ... & w^{l}_{k2}\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w^{l}_{1j} & w^{l}_{2j} & ... & w^{l}_{kj}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    \\delta^{l}_{1} \\\\\n",
    "    \\delta^{l}_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\delta^{l}_{k}\n",
    "    \\end{bmatrix}\n",
    "    \\;\\odot\\;\n",
    "    \\begin{bmatrix}\n",
    "    \\sigma'(z^{l - 1}_{1}) \\\\\n",
    "    \\sigma'(z^{l - 1}_{2}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sigma'(z^{l - 1}_{j}) \\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "    $$\n",
    "    \\large\n",
    "    \\Leftrightarrow\n",
    "    \\delta^{l - 1} = (W^{l})^{T} \\delta^{l} \\odot \\sigma'(z^{l - 1})\n",
    "    $$\n",
    "#### `Step 5: Gradient Descent`\n",
    "- `Gradient`:\n",
    "    $$\n",
    "    \\large\n",
    "    \\begin{cases}\n",
    "    \\frac{\\partial C}{\\partial w^{l}_{jk}} = \\delta^{l}_{j} a^{l - 1}_{k} \\\\\n",
    "    \\frac{\\partial C}{\\partial b^{l}_{j}} = \\delta^{l}_{j}\n",
    "    \\end{cases}\n",
    "    \\Rightarrow\n",
    "    \\begin{cases}\n",
    "    \\frac{\\partial C}{\\partial W^{l}} = \\delta^{l} (a^{l - 1})^T \\\\\n",
    "    \\frac{\\partial C}{\\partial b^{l}} = \\delta^{l}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "- `Update`:\n",
    "    $$\n",
    "    \\large\n",
    "    \\begin{cases}\n",
    "    W^{l} := W^{l} - \\alpha \\frac{\\partial C}{\\partial W^{l}} = W^{l} - \\alpha \\delta^{l}(a^{l - 1})^T \\\\\n",
    "    b^{l} := b^{l} - \\alpha \\frac{\\partial C}{\\partial b^{l}} = b^{l} - \\alpha \\delta^{l}\n",
    "    \\end{cases}\n",
    "    $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
